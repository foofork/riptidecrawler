# RipTide CLI Specification v1.0.0
# Single source of truth for all CLI behavior - commands, flags, help text, exit codes

version: "1.0.0"
name: "riptide"
about: "High-performance web crawler and content extraction CLI"
author: "RipTide Team <team@riptide.dev>"

# ===== GLOBAL CONFIGURATION =====
config:
  precedence: ["flags", "env", "config_file"]  # flags > env > ~/.config/riptide/config.yaml
  config_path: "~/.config/riptide/config.yaml"
  base_url:
    default: "http://localhost:8080"
    env: "RIPTIDE_BASE_URL"
    flag: "--url"

# ===== GLOBAL FLAGS =====
# Available to all commands
global_flags:
  - name: url
    long: --url
    short: -u
    env: RIPTIDE_BASE_URL
    default: "http://localhost:8080"
    help: "RipTide API server URL"
    type: string

  - name: api-key
    long: --api-key
    short: -k
    env: RIPTIDE_API_KEY
    help: "API authentication key"
    type: string

  - name: output
    long: --output
    short: -o
    values: [json, table, text, ndjson]
    default: text
    help: "Output format"
    type: string

  - name: quiet
    long: --quiet
    short: -q
    type: bool
    default: false
    help: "Suppress progress output (stderr)"

  - name: verbose
    long: --verbose
    short: -v
    type: bool
    default: false
    help: "Verbose output"

# ===== EXIT CODES =====
exit_codes:
  success: 0
  user_error: 1      # Invalid args, config, network issues
  server_error: 2    # 5xx responses, protocol errors
  invalid_args: 3    # clap validation failure

# ===== COMMANDS =====
commands:

  # ===== EXTRACT =====
  # Advanced extraction with full strategy control
  - name: extract
    about: "Extract content from URLs with strategy control"
    api:
      method: POST
      path: "/extract"

    args:
      - name: urls
        type: positional
        required: true
        multiple: true
        help: "URLs to extract content from"

    flags:
      - name: strategy
        long: --strategy
        short: -s
        values: [auto, css, wasm, llm, multi]
        default: multi
        help: "Extraction strategy (auto/css/wasm/llm/multi)"
        type: string

      - name: selector
        long: --selector
        help: "CSS selector for content extraction (css strategy)"
        type: string

      - name: pattern
        long: --pattern
        help: "Regex pattern for content extraction"
        type: string

      - name: quality-threshold
        long: --quality-threshold
        short: -q
        type: float
        default: 0.7
        help: "Minimum quality threshold (0.0-1.0)"

      - name: timeout
        long: --timeout
        short: -t
        type: int
        default: 30
        help: "Extraction timeout (seconds)"

      - name: concurrency
        long: --concurrency
        short: -c
        type: int
        default: 5
        help: "Concurrent extraction requests"

      - name: cache
        long: --cache
        values: [auto, read_write, read_only, write_only, disabled]
        default: auto
        help: "Cache mode"
        type: string

      - name: output-file
        long: --output-file
        short: -f
        type: path
        help: "Save results to file"

    examples:
      - command: "riptide extract https://example.com"
        description: "Extract with default multi-strategy"

      - command: "riptide extract https://example.com --strategy css --selector 'article'"
        description: "Extract using CSS selector"

      - command: "riptide extract https://example.com --strategy llm --quality-threshold 0.9"
        description: "High-quality LLM extraction"

      - command: "riptide extract https://example.com https://example.org --strategy wasm -c 10"
        description: "Batch extraction with WASM strategy"

  # ===== SPIDER =====
  # Deep crawl with automatic extraction (no strategy control)
  - name: spider
    about: "Deep crawl with spider engine (automatic extraction)"
    api:
      method: POST
      path: "/spider/crawl"

    args:
      - name: url
        type: positional
        required: true
        help: "Starting URL for spider crawl"

    flags:
      - name: depth
        long: --depth
        short: -d
        type: int
        default: 3
        help: "Maximum crawl depth"

      - name: pages
        long: --pages
        short: -p
        type: int
        default: 100
        help: "Maximum pages to crawl"

      - name: strategy
        long: --strategy
        values: [breadth_first, depth_first, best_first]
        default: breadth_first
        help: "Crawl strategy"
        type: string

      - name: concurrency
        long: --concurrency
        short: -c
        type: int
        default: 5
        help: "Concurrent requests"

      - name: timeout
        long: --timeout
        short: -t
        type: int
        default: 30
        help: "Request timeout (seconds)"

      - name: cache
        long: --cache
        values: [auto, read_write, read_only, write_only, disabled]
        default: auto
        help: "Cache mode"
        type: string

      - name: output-file
        long: --output-file
        short: -f
        type: path
        help: "Save results to file"

      - name: robots
        long: --robots
        values: [respect, ignore]
        default: respect
        help: "robots.txt handling"
        type: string

    examples:
      - command: "riptide spider https://docs.example.com"
        description: "Spider crawl with defaults"

      - command: "riptide spider https://docs.example.com --depth 5 --pages 500"
        description: "Deep crawl documentation site"

      - command: "riptide spider https://example.com --strategy depth_first -c 10"
        description: "Depth-first crawl with high concurrency"

  # ===== SEARCH =====
  # Deep search with automatic content extraction
  - name: search
    about: "Deep search with content extraction (automatic extraction)"
    api:
      method: POST
      path: "/deepsearch"
      streaming_variant: "/deepsearch/stream"

    args:
      - name: query
        type: positional
        required: true
        help: "Search query"

    flags:
      - name: limit
        long: --limit
        short: -l
        type: int
        default: 10
        help: "Maximum results"

      - name: stream
        long: --stream
        type: bool
        default: false
        help: "Stream results as NDJSON"

      - name: include-content
        long: --include-content
        type: bool
        default: false
        help: "Extract full content from results"

      - name: timeout
        long: --timeout
        short: -t
        type: int
        default: 30
        help: "Search timeout (seconds)"

      - name: output-file
        long: --output-file
        short: -f
        type: path
        help: "Save results to file"

    examples:
      - command: 'riptide search "web scraping"'
        description: "Basic search"

      - command: 'riptide search "web scraping" --limit 20 --include-content'
        description: "Search with content extraction"

      - command: 'riptide search "rust tutorials" --stream -o results.jsonl'
        description: "Stream search results to file"

  # ===== RENDER =====
  # JavaScript-heavy page rendering
  - name: render
    about: "Render JavaScript-heavy pages with headless browser"
    api:
      method: POST
      path: "/render"

    args:
      - name: urls
        type: positional
        required: true
        multiple: true
        help: "URLs to render"

    flags:
      - name: wait
        long: --wait
        short: -w
        type: int
        default: 2000
        help: "Wait time in milliseconds after page load"

      - name: screenshot
        long: --screenshot
        type: bool
        default: false
        help: "Capture screenshot of rendered page"

      - name: viewport
        long: --viewport
        default: "1920x1080"
        help: "Viewport size (WIDTHxHEIGHT)"
        type: string

      - name: timeout
        long: --timeout
        short: -t
        type: int
        default: 30
        help: "Render timeout (seconds)"

      - name: output-file
        long: --output-file
        short: -f
        type: path
        help: "Save results to file"

    examples:
      - command: "riptide render https://spa.example.com"
        description: "Render single-page application"

      - command: "riptide render https://spa.example.com --wait 5000 --screenshot"
        description: "Render with longer wait and screenshot"

      - command: "riptide render https://app.example.com --viewport 1024x768"
        description: "Render with custom viewport"

  # ===== DOCTOR =====
  # System health diagnostics
  - name: doctor
    about: "System health diagnostics and troubleshooting"
    api:
      method: GET
      path: "/healthz"

    flags:
      - name: full
        long: --full
        type: bool
        default: false
        help: "Full diagnostic report including Redis, headless pool, DNS"

      - name: json
        long: --json
        type: bool
        default: false
        help: "Output detailed JSON diagnostics"

    logic:
      # Special handling: check multiple endpoints and provide remediation
      checks:
        - name: api_server
          endpoint: /healthz
          critical: true
          remediation: "Start API server with: ./target/release/riptide-api"

        - name: redis
          endpoint: /healthz
          field: dependencies.redis.status
          critical: true
          remediation: "Start Redis with: docker run -d -p 6379:6379 redis:7-alpine"

        - name: headless_pool
          endpoint: /healthz
          field: dependencies.headless_service.status
          critical: false
          remediation: "JavaScript pages will fail. Start Chrome pool with docker-compose."

        - name: outbound_dns
          test: "resolve example.com"
          critical: false
          remediation: "Check network connectivity and DNS configuration"

    examples:
      - command: "riptide doctor"
        description: "Quick health check"

      - command: "riptide doctor --full"
        description: "Comprehensive system diagnostics"

      - command: "riptide doctor --full --json"
        description: "Full diagnostics in JSON format"

  # ===== CONFIG =====
  # Configuration management (local operations, no API)
  - name: config
    about: "Manage configuration settings"
    api:
      method: LOCAL
      path: "~/.config/riptide/config.yaml"

    subcommands:
      - name: get
        about: "Get configuration value"
        args:
          - name: key
            type: positional
            required: true
            help: "Configuration key (e.g., base_url, api_key)"

      - name: set
        about: "Set configuration value"
        args:
          - name: key
            type: positional
            required: true
            help: "Configuration key"
          - name: value
            type: positional
            required: true
            help: "Configuration value"

      - name: list
        about: "List all configuration settings"

      - name: reset
        about: "Reset configuration to defaults"

      - name: path
        about: "Show configuration file path"

    examples:
      - command: "riptide config get base_url"
        description: "Get base URL setting"

      - command: "riptide config set base_url http://localhost:8080"
        description: "Set base URL"

      - command: "riptide config set api_key sk-..."
        description: "Set API key"

      - command: "riptide config list"
        description: "List all settings"

      - command: "riptide config reset"
        description: "Reset to defaults"

  # ===== SESSION =====
  # Session management for multi-step workflows
  - name: session
    about: "Manage extraction sessions for multi-step workflows"
    api:
      base_path: "/sessions"

    subcommands:
      - name: create
        about: "Create new session"
        api:
          method: POST
          path: "/sessions"
        flags:
          - name: name
            long: --name
            short: -n
            type: string
            help: "Session name"
          - name: ttl
            long: --ttl
            type: int
            default: 3600
            help: "Session TTL in seconds"

      - name: list
        about: "List all sessions"
        api:
          method: GET
          path: "/sessions"

      - name: get
        about: "Get session details"
        api:
          method: GET
          path: "/sessions/{id}"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"

      - name: delete
        about: "Delete session"
        api:
          method: DELETE
          path: "/sessions/{id}"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"

      - name: add
        about: "Add URL to session"
        api:
          method: POST
          path: "/sessions/{id}/urls"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"
          - name: url
            type: positional
            required: true
            help: "URL to add"

      - name: extract
        about: "Extract content from session URLs"
        api:
          method: POST
          path: "/sessions/{id}/extract"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"
        flags:
          - name: strategy
            long: --strategy
            values: [auto, css, wasm, llm, multi]
            default: multi
            help: "Extraction strategy"
            type: string

      - name: results
        about: "Get session extraction results"
        api:
          method: GET
          path: "/sessions/{id}/results"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"

      - name: export
        about: "Export session data"
        api:
          method: GET
          path: "/sessions/{id}/export"
        args:
          - name: session_id
            type: positional
            required: true
            help: "Session ID"
        flags:
          - name: format
            long: --format
            values: [json, csv, ndjson]
            default: json
            help: "Export format"
            type: string

    examples:
      - command: "riptide session create --name research"
        description: "Create named session"

      - command: "riptide session add <session-id> https://example.com"
        description: "Add URL to session"

      - command: "riptide session extract <session-id> --strategy llm"
        description: "Extract content from session URLs"

      - command: "riptide session results <session-id>"
        description: "View extraction results"

      - command: "riptide session export <session-id> --format csv"
        description: "Export session data as CSV"

      - command: "riptide session list"
        description: "List all active sessions"

      - command: "riptide session delete <session-id>"
        description: "Delete session"

# ===== HTTP ERROR TO EXIT CODE MAPPING =====
error_mapping:
  # 4xx errors -> exit 1 (user/config error)
  400: 1  # Bad Request
  401: 1  # Unauthorized
  403: 1  # Forbidden
  404: 1  # Not Found
  429: 1  # Too Many Requests

  # 5xx errors -> exit 2 (server/protocol error)
  500: 2  # Internal Server Error
  502: 2  # Bad Gateway
  503: 2  # Service Unavailable
  504: 2  # Gateway Timeout

  # Network errors -> exit 1
  connection_refused: 1
  timeout: 1
  dns_failed: 1
  invalid_response: 2

# ===== OUTPUT FORMAT TEMPLATES =====
output_formats:
  extract_success:
    json: '{"results": [...], "summary": {"total": N, "successful": N, "failed": N}}'
    text: |
      ✓ Extracted {total} URLs

      Successful: {successful}
      Failed: {failed}
      Strategy: {strategy}
      Quality: {avg_quality}

    table: |
      | URL | Status | Strategy | Quality | Size |
      |-----|--------|----------|---------|------|

  spider_success:
    json: '{"results": [...], "summary": {"pages_crawled": N, "depth_reached": N, "links_found": N}}'
    text: |
      ✓ Spider crawl complete

      Pages crawled: {pages_crawled}
      Max depth reached: {depth_reached}
      Links found: {links_found}
      Time: {elapsed}

  search_success:
    json: '{"results": [...], "summary": {"total_results": N, "query": "..."}}'
    text: |
      ✓ Search results for "{query}"

      Found: {total_results} results
      {results}

  doctor_success:
    text: |
      ✓ RipTide Health Check

      API Server: {api_status}
      Redis: {redis_status}
      Headless Pool: {headless_status}
      DNS: {dns_status}

  doctor_failure:
    text: |
      ✗ RipTide Health Issues Detected

      {issues}

      Remediation:
      {remediation_steps}

  session_created:
    json: '{"session_id": "...", "name": "...", "created_at": "...", "ttl": N}'
    text: |
      ✓ Session created

      ID: {session_id}
      Name: {name}
      TTL: {ttl} seconds

# ===== VALIDATION RULES =====
validation:
  url:
    pattern: "^https?://.+"
    error: "URL must start with http:// or https://"

  quality_threshold:
    min: 0.0
    max: 1.0
    error: "Quality threshold must be between 0.0 and 1.0"

  concurrency:
    min: 1
    max: 100
    error: "Concurrency must be between 1 and 100"

  timeout:
    min: 1
    max: 300
    error: "Timeout must be between 1 and 300 seconds"

  depth:
    min: 1
    max: 20
    error: "Depth must be between 1 and 20"

  pages:
    min: 1
    max: 10000
    error: "Pages must be between 1 and 10000"

# ===== ENVIRONMENT VARIABLES =====
environment_variables:
  RIPTIDE_BASE_URL:
    description: "API server base URL"
    default: "http://localhost:8080"

  RIPTIDE_API_KEY:
    description: "API authentication key"

  RIPTIDE_CONCURRENCY:
    description: "Default concurrency level"
    default: "5"

  RIPTIDE_TIMEOUT:
    description: "Default request timeout in seconds"
    default: "30"

  RIPTIDE_CACHE_MODE:
    description: "Default cache mode"
    default: "auto"
    values: [auto, read_write, read_only, write_only, disabled]

  RIPTIDE_PROXY:
    description: "HTTP/HTTPS proxy URL"

  RIPTIDE_CONFIG_PATH:
    description: "Custom configuration file path"
    default: "~/.config/riptide/config.yaml"

# ===== HELP TEXT TEMPLATES =====
help_templates:
  command_header: |
    {name} {version}
    {about}

  command_usage: |
    USAGE:
        {bin} {command} [OPTIONS] {args}

  global_options: |
    GLOBAL OPTIONS:
        {flags}

  examples_header: |
    EXAMPLES:

  examples_format: |
        {command}
            {description}

# ===== STREAMING SUPPORT =====
streaming:
  supported_commands: [search, extract]

  format: ndjson

  progress_indicators:
    enabled: true
    style: "dots"  # dots, bar, spinner

  error_handling:
    continue_on_error: true
    report_errors_inline: true
