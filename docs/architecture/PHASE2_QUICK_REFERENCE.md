# Phase 2 Quick Reference Card

## ğŸ¯ 5 Result Modes

```rust
Stats   â†’ JSON stats only             (existing, backward compatible)
Urls    â†’ JSON stats + URL list       (existing, Phase 1)
Pages   â†’ JSON stats + full pages     (NEW, max 1000 pages)
Stream  â†’ NDJSON/SSE real-time        (NEW, unlimited pages)
Store   â†’ Async job + pagination      (NEW, server-side storage)
```

## ğŸ“Š CrawledPage Fields (18 total)

**Core (always included):**
```rust
url: String
depth: u32
status_code: u16
```

**Heavy (gated by field selection):**
```rust
content: Option<String>     // Raw HTML (1MB limit)
markdown: Option<String>    // Normalized markdown
```

**Metadata:**
```rust
title: Option<String>
links: Vec<String>
final_url: Option<String>
canonical_url: Option<String>
mime: Option<String>
charset: Option<String>
fetch_time_ms: u64
crawled_at: String (ISO 8601)
```

**Compliance:**
```rust
robots_obeyed: bool
disallowed: bool
```

**Debugging:**
```rust
fetch_error: Option<String>
parse_error: Option<String>
truncated: bool
```

## ğŸ”§ API Endpoints

### Enhanced Spider Crawl
```bash
POST /spider/crawl?result_mode=pages&include=title,links&exclude=content
```

**Query Parameters:**
- `result_mode`: stats|urls|pages|stream|store (default: stats)
- `include`: comma-separated field names
- `exclude`: comma-separated field names
- `max_pages`: max pages for Pages mode (default: 1000, max: 10000)

### Job Storage (NEW)
```bash
# Create async job
POST /spider/crawl?result_mode=store
â†’ {"job_id": "uuid", "status": "running"}

# Get results with pagination
GET /jobs/{id}/results?cursor=12345&limit=100&include=title,links
â†’ {"pages": [...], "cursor": 12346, "has_more": true}

# Get job stats
GET /jobs/{id}/stats
â†’ {"status": "completed", "pages_crawled": 542, ...}
```

### Extraction Helpers (NEW)
```bash
# Batch extract
POST /extract/batch
{"urls": ["url1", "url2"], "format": "markdown", "concurrency": 10}

# Spider + Extract
POST /spider+extract
{"seeds": ["..."], "extract_pattern": ".*\\/article\\/.*"}
```

## ğŸ¨ Field Selection Examples

```bash
# Lightweight (exclude heavy fields)
?include=title,links

# Full page (include everything)
?include=url,depth,status_code,title,content,markdown,links

# Exclude only content
?exclude=content

# Default (no params)
# â†’ includes all except content,markdown
```

## ğŸ“¡ Streaming Protocols

**NDJSON:**
```bash
curl -H "Accept: application/x-ndjson" /spider/crawl?result_mode=stream
```
```json
{"type":"page","data":{"url":"...","title":"..."}}
{"type":"page","data":{"url":"...","title":"..."}}
{"type":"stats","data":{"pages_crawled":2}}
```

**SSE:**
```bash
curl -H "Accept: text/event-stream" /spider/crawl?result_mode=stream
```
```
data: {"type":"page","data":{"url":"..."}}

data: {"type":"stats","data":{"pages_crawled":2}}

```

## ğŸ›¡ï¸ Safety Limits

```rust
max_pages_per_request: 1000     // Pages mode limit
max_content_bytes: 1MB          // Per-page truncation
max_discovered_urls: 10,000     // URL list limit
max_stored_jobs_per_user: 100   // Concurrent jobs
job_retention_days: 30          // Storage duration
```

## ğŸš¨ Error Codes

```
VALIDATION_ERROR     â†’ Invalid field names or parameters
QUOTA_EXCEEDED       â†’ Too many concurrent jobs
RESULT_TOO_LARGE     â†’ Use Stream or Store mode instead
```

## ğŸ“ Response Examples

**Pages Mode:**
```json
{
  "pages_crawled": 42,
  "pages_failed": 3,
  "duration_seconds": 12.5,
  "stop_reason": "max_pages",
  "api_version": "1.0",
  "pages": [
    {
      "url": "https://example.com",
      "depth": 0,
      "status_code": 200,
      "title": "Example Domain",
      "links": ["https://example.com/page1"],
      "fetch_time_ms": 245,
      "truncated": false,
      "crawled_at": "2025-10-29T12:00:00Z"
    }
  ]
}
```

**Stream Mode (NDJSON):**
```json
{"type":"page","data":{"url":"https://example.com","depth":0}}
{"type":"page","data":{"url":"https://example.com/page1","depth":1}}
{"type":"stats","data":{"pages_crawled":2,"pages_failed":0}}
```

**Store Mode:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "running",
  "created_at": "2025-10-29T12:00:00Z"
}
```

## ğŸ—‚ï¸ Database Schema

**spider_jobs:**
```sql
job_id (UUID), status, created_at, completed_at,
pages_crawled, pages_failed, duration_seconds, stop_reason
```

**spider_pages:**
```sql
page_id (BIGSERIAL), job_id (FK), url, title, content,
markdown, links (JSONB), depth, status_code, ...
```

## ğŸ”„ Workflow: Spider + Extract

```
1. Crawl â†’ Discover URLs
2. Filter â†’ Apply regex pattern
3. Extract â†’ Batch markdown extraction
4. Combine â†’ Crawl metadata + extracted content
```

```bash
POST /spider+extract
{
  "seeds": ["https://example.com"],
  "scope": {"max_depth": 2},
  "extract_pattern": ".*\\/article\\/.*",
  "include": ["markdown", "title"]
}
```

## ğŸ“š Implementation Files

**Core Structs:**
- `riptide-api/src/dto.rs` â†’ CrawledPage, SpiderResultPages, ResultMode

**Handlers:**
- `riptide-api/src/handlers/spider_pages.rs` â†’ PageBuilder
- `riptide-api/src/handlers/spider_stream.rs` â†’ NDJSON/SSE
- `riptide-api/src/handlers/spider_jobs.rs` â†’ Job storage
- `riptide-api/src/handlers/extract_batch.rs` â†’ Batch extract
- `riptide-api/src/handlers/spider_extract.rs` â†’ Spider+extract

**Migrations:**
- `migrations/001_spider_jobs.sql`
- `migrations/002_spider_pages.sql`

## ğŸ¯ Decision Flow

```
result_mode=pages â†’ Check page count
  â”œâ”€ <= 1000 pages â†’ Return JSON
  â””â”€ > 1000 pages â†’ Error: Use Stream/Store

result_mode=stream â†’ Content-Type negotiation
  â”œâ”€ Accept: application/x-ndjson â†’ NDJSON
  â””â”€ Accept: text/event-stream â†’ SSE

result_mode=store â†’ Create job
  â”œâ”€ Spawn background task
  â”œâ”€ Store to database
  â””â”€ Return job_id
```

## âš™ï¸ Configuration

```rust
// In config
pub struct SpiderLimits {
    pub max_pages_per_request: usize,
    pub max_content_bytes: usize,
    pub max_discovered_urls: usize,
    pub max_stored_jobs_per_user: usize,
    pub job_retention_days: u32,
}
```

## ğŸ§ª Testing Checklist

- [ ] Field selection filters correctly
- [ ] Content truncation sets `truncated: true`
- [ ] NDJSON streaming emits final stats
- [ ] SSE streaming handles disconnects
- [ ] Job pagination cursor works
- [ ] Per-page errors don't fail crawl
- [ ] Quota enforcement blocks excess jobs
- [ ] Response size limits trigger errors

## ğŸ“– Full Documentation

**Main Architecture:** `/workspaces/eventmesh/docs/architecture/phase2-api-design.md`
**Data Flows:** `/workspaces/eventmesh/docs/architecture/phase2-data-flow.md`
**Summary:** `/workspaces/eventmesh/docs/architecture/PHASE2_ARCHITECTURE_SUMMARY.md`
**Completion Report:** `/workspaces/eventmesh/docs/architecture/PHASE2_COMPLETION_REPORT.md`

---

**Quick Start:** Read phase2-api-design.md Section 3 (Core Data Structures) â†’ Section 4 (Result Mode Design) â†’ Section 11 (Implementation Plan)
