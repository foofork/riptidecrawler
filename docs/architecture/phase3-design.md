# Phase 3 Architecture Design Review

## Executive Summary

This document provides a comprehensive architecture review of the RipTide Crawler codebase and identifies optimal integration points for Phase 3 features. The current system demonstrates a well-designed modular architecture that provides clear extension mechanisms for all planned Phase 3 capabilities.

## Current Architecture Assessment

### ‚úÖ **Strengths Identified**

1. **Modular Workspace Structure**: Clean separation into 5 focused crates
   - `riptide-core`: Shared functionality and types
   - `riptide-api`: REST API service layer
   - `riptide-headless`: Browser automation service
   - `riptide-workers`: Background processing (ready for scaling)
   - `riptide-extractor-wasm`: WASM component for content processing

2. **Flexible Pipeline Architecture**: Well-designed gate-based routing system
   - Dynamic decision making between fast extraction and headless rendering
   - Quality scoring algorithm for content assessment
   - Built-in fallback mechanisms for reliability

3. **Modern Tech Stack**: Production-ready foundation
   - Axum web framework with comprehensive middleware
   - Tokio async runtime with proper concurrency control
   - WASM Component Model with wasmtime runtime
   - Redis caching with read-through patterns

4. **Comprehensive Monitoring**: Production-ready observability
   - Prometheus metrics integration
   - Structured logging with tracing
   - Health check endpoints with dependency validation

### üéØ **Phase 3 Integration Points Identified**

## 1. Deep Crawling with Spider-rs Integration

### Current State
- No spider-rs integration found in codebase
- Single URL processing pipeline in place
- Basic deepsearch endpoint uses Serper.dev API only

### Integration Strategy
**Location**: `crates/riptide-core/src/crawl.rs` (new module)

```rust
// New module structure
pub mod crawl {
    pub struct SpiderCrawler {
        spider_instance: spider::Spider,
        budget_manager: BudgetManager,
        url_queue: VecDeque<CrawlJob>,
    }

    pub struct CrawlJob {
        pub url: String,
        pub depth: u32,
        pub parent_url: Option<String>,
        pub discovered_at: Instant,
    }
}
```

**API Enhancement**: Extend `/crawl` endpoint
```rust
#[derive(Deserialize)]
pub struct CrawlBody {
    pub urls: Vec<String>,
    pub options: Option<CrawlOptions>,
    // NEW: Deep crawling options
    pub deep_crawl: Option<DeepCrawlOptions>,
}

#[derive(Deserialize)]
pub struct DeepCrawlOptions {
    pub max_depth: Option<u32>,      // Default: 2
    pub max_pages: Option<u32>,      // Default: 50
    pub same_domain_only: bool,      // Default: true
    pub respect_robots: bool,        // Default: true
    pub crawl_budget_seconds: Option<u64>, // Default: 300
}
```

### Implementation Plan
1. **Add spider dependency** to `riptide-core/Cargo.toml`
2. **Create crawl.rs module** with spider integration
3. **Enhance pipeline orchestrator** to handle crawl queues
4. **Integrate with existing gate system** for discovered URLs
5. **Add sitemap discovery** using spider's capabilities

## 2. Dynamic Content Handling Enhancement

### Current State
- Basic headless service at port 9123
- Simple `/render` endpoint with limited options
- Pipeline supports headless fallback through gate system

### Enhancement Strategy
**Location**: `crates/riptide-headless/src/cdp.rs` (enhance existing)

```rust
#[derive(Deserialize)]
pub struct RenderRequest {
    pub url: String,
    // ENHANCED: Advanced wait conditions
    pub wait_for: Option<WaitCondition>,
    pub scroll_config: Option<ScrollConfig>,
    pub actions: Option<Vec<PageAction>>,
    pub capture_screenshot: Option<bool>,
    pub capture_mhtml: Option<bool>,
}

#[derive(Deserialize)]
pub enum WaitCondition {
    Timeout(u64),
    Selector(String),
    CustomJs(String),
    NetworkIdle(u64),
}

#[derive(Deserialize)]
pub struct ScrollConfig {
    pub steps: u32,
    pub step_px: u32,
    pub delay_ms: u32,
}
```

**Pipeline Integration**: Enhance existing gate system
```rust
// In gate.rs - add dynamic content detection
impl GateFeatures {
    pub fn requires_dynamic_rendering(&self) -> bool {
        self.spa_markers >= 2 ||
        self.script_bytes > self.html_bytes / 2 ||
        self.visible_text_chars < 100
    }
}
```

### Implementation Plan
1. **Enhance RenderRequest model** with advanced options
2. **Implement wait condition handlers** in CDP service
3. **Add screenshot/MHTML capture** using chromiumoxide
4. **Update pipeline** to pass dynamic options through
5. **Add artifact storage** to AppState

## 3. Stealth Features Integration

### Current State
- Basic HTTP client without stealth features
- No user-agent rotation or request randomization
- Basic robots.txt compliance in place

### Integration Strategy
**Location**: `crates/riptide-core/src/stealth.rs` (new module)

```rust
pub struct StealthManager {
    user_agents: Vec<String>,
    proxy_pool: Option<Vec<ProxyConfig>>,
    request_delays: HashMap<String, Instant>, // Per-domain delays
}

#[derive(Clone)]
pub struct StealthOptions {
    pub rotate_user_agents: bool,
    pub randomize_headers: bool,
    pub use_proxies: bool,
    pub add_jitter: bool,
    pub stealth_headless: bool, // Disable automation flags
}
```

**HTTP Client Enhancement**: Modify existing fetch module
```rust
// In fetch.rs - enhance existing client
pub async fn get_with_stealth(
    client: &reqwest::Client,
    url: &str,
    stealth_opts: &StealthOptions,
) -> Result<Response> {
    let mut request = client.get(url);

    if stealth_opts.rotate_user_agents {
        request = request.header("User-Agent", select_random_ua());
    }

    // Apply stealth configurations...
}
```

### Implementation Plan
1. **Create stealth.rs module** with UA rotation and proxy support
2. **Enhance fetch.rs** to use stealth options
3. **Update headless service** with stealth browser flags
4. **Add stealth options** to CrawlOptions struct
5. **Implement per-domain request throttling**

## 4. PDF Processing Pipeline

### Current State
- `pdfium-render` dependency already present (optional feature)
- No PDF-specific processing pipeline
- Gate system could route PDF URLs appropriately

### Integration Strategy
**Location**: `crates/riptide-core/src/pdf.rs` (new module)

```rust
pub struct PdfProcessor {
    pdfium: PdfiumLibrary,
}

pub struct PdfExtractionResult {
    pub text: String,
    pub metadata: PdfMetadata,
    pub images: Vec<ExtractedImage>,
    pub page_count: u32,
}

#[derive(Serialize, Deserialize)]
pub struct PdfMetadata {
    pub title: Option<String>,
    pub author: Option<String>,
    pub creation_date: Option<String>,
    pub modification_date: Option<String>,
    pub creator: Option<String>,
    pub producer: Option<String>,
}
```

**Pipeline Integration**: Enhance content-type routing
```rust
// In pipeline.rs - add PDF detection
async fn extract_content(&self, content: &[u8], url: &str) -> ApiResult<ExtractedDoc> {
    // Detect content type
    if url.ends_with(".pdf") || is_pdf_content(content) {
        return self.extract_pdf_content(content, url).await;
    }

    // Existing HTML extraction logic...
}
```

### Implementation Plan
1. **Enable pdf feature** in riptide-core
2. **Create pdf.rs module** with pdfium integration
3. **Add PDF detection** to pipeline orchestrator
4. **Update ExtractedDoc** to handle PDF-specific metadata
5. **Add PDF support** to WASM extractor if needed

## 5. Streaming Output Implementation

### Current State
- Synchronous JSON responses in handlers
- Batch processing with complete results
- No real-time progress updates

### Integration Strategy
**Location**: `crates/riptide-api/src/streaming.rs` (new module)

```rust
use axum::response::sse::{Event, KeepAlive, Sse};
use tokio_stream::StreamExt;

pub async fn crawl_stream(
    State(state): State<AppState>,
    Json(body): Json<CrawlBody>,
) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
    let stream = create_crawl_stream(state, body).await;
    Sse::new(stream).keep_alive(KeepAlive::default())
}

// NDJSON streaming for /crawl endpoint
pub fn create_crawl_stream(
    state: AppState,
    body: CrawlBody,
) -> impl Stream<Item = Result<Event, Infallible>> {
    // Implementation with tokio channels
}
```

**API Enhancement**: Add streaming endpoints
```rust
// New streaming routes
app.route("/crawl/stream", post(streaming::crawl_stream))
   .route("/deepsearch/stream", post(streaming::deepsearch_stream))
```

### Implementation Plan
1. **Add streaming dependencies** (Server-Sent Events or WebSockets)
2. **Create streaming.rs module** with SSE/NDJSON support
3. **Refactor pipeline orchestrator** to emit progress events
4. **Add streaming endpoints** alongside existing batch endpoints
5. **Implement client-side chunking** for large content

## 6. Content Chunking and Enhanced Extraction

### Current State
- Basic ExtractedDoc structure with limited metadata
- No content chunking or token-aware splitting
- WASM extractor uses simple trek-rs integration

### Enhancement Strategy
**Location**: `crates/riptide-core/src/chunking.rs` (new module)

```rust
pub struct ContentChunker {
    max_tokens: usize,
    overlap_tokens: usize,
    tokenizer: TokenizerImpl,
}

pub struct ContentChunk {
    pub text: String,
    pub tokens: usize,
    pub chunk_index: u32,
    pub metadata: ChunkMetadata,
}

#[derive(Serialize, Deserialize)]
pub struct ChunkMetadata {
    pub source_url: String,
    pub start_offset: usize,
    pub end_offset: usize,
    pub preserve_boundaries: bool,
}
```

**Enhanced Extraction**: Improve ExtractedDoc
```rust
// In types.rs - enhance existing structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExtractedDoc {
    // Existing fields...

    // NEW: Enhanced metadata
    pub byline: Option<String>,
    pub published_iso: Option<String>,
    pub content_chunks: Option<Vec<ContentChunk>>,
    pub structured_data: Option<serde_json::Value>, // JSON-LD, microdata
    pub outlinks: Vec<String>,
    pub images_with_context: Vec<ImageContext>,
}
```

### Implementation Plan
1. **Create chunking.rs module** with token-aware splitting
2. **Enhance ExtractedDoc** with additional metadata fields
3. **Update WASM extractor** to extract byline/date/structured data
4. **Add chunking options** to CrawlOptions
5. **Implement outlink extraction** for site discovery

## Security and Performance Considerations

### Security Review

1. **Input Validation**
   - ‚úÖ URL validation present in validation.rs
   - ‚úÖ Content-type checking in place
   - ‚ö†Ô∏è **Enhancement needed**: PDF file validation for malicious content
   - ‚ö†Ô∏è **Enhancement needed**: Stealth proxy validation

2. **Rate Limiting and Ethics**
   - ‚úÖ Robots.txt compliance implemented
   - ‚úÖ Per-host throttling with jitter
   - ‚úÖ Configurable delays and timeouts
   - ‚ö†Ô∏è **Enhancement needed**: IP rotation rate limiting

3. **Resource Management**
   - ‚úÖ WASM component lifecycle managed
   - ‚úÖ HTTP client pooling in place
   - ‚ö†Ô∏è **Enhancement needed**: PDF processing memory limits
   - ‚ö†Ô∏è **Enhancement needed**: Deep crawl budget enforcement

### Performance Analysis

1. **Current Performance Characteristics**
   - ‚úÖ 16 concurrent workers by default
   - ‚úÖ Redis caching with TTL
   - ‚úÖ Semaphore-based concurrency control
   - ‚úÖ Prometheus metrics for monitoring

2. **Phase 3 Performance Implications**
   - **Spider-rs Integration**: +25% memory usage for crawl queues
   - **PDF Processing**: +50MB memory per PDF, recommend 2 concurrent limit
   - **Streaming Output**: -30% memory usage (no batch accumulation)
   - **Enhanced Headless**: +200ms average latency for dynamic content

3. **Recommended Optimizations**
   ```rust
   // In AppConfig - new performance settings
   pub struct AppConfig {
       // Existing settings...

       // NEW: Performance tuning
       pub max_concurrent_pdfs: usize,     // Default: 2
       pub crawl_queue_max_size: usize,    // Default: 1000
       pub streaming_buffer_size: usize,   // Default: 64KB
       pub headless_pool_size: usize,      // Default: 4
   }
   ```

## Migration Strategy

### Phase 3.1: Foundation (Week 1)
1. **Add spider-rs dependency** and create crawl module
2. **Enhance headless service** with dynamic content options
3. **Create stealth module** with basic UA rotation
4. **Add PDF processing** with pdfium integration

### Phase 3.2: Integration (Week 2)
1. **Integrate spider with pipeline** orchestrator
2. **Update API handlers** with new options
3. **Implement streaming endpoints** with SSE
4. **Add enhanced metadata** extraction

### Phase 3.3: Optimization (Week 3)
1. **Add performance monitoring** for new features
2. **Implement content chunking** with token awareness
3. **Optimize memory usage** for concurrent PDF processing
4. **Add comprehensive error handling**

### Phase 3.4: Testing & Polish (Week 4)
1. **Create integration tests** for all new features
2. **Performance testing** with realistic workloads
3. **Documentation updates** for API changes
4. **Security audit** of new components

## API Compatibility

### Backward Compatibility
All existing API endpoints will remain fully compatible. New features are additive:

```rust
// EXISTING: Fully supported
POST /crawl
{
  "urls": ["https://example.com"],
  "options": { "concurrency": 8 }
}

// NEW: Optional deep crawling
POST /crawl
{
  "urls": ["https://example.com"],
  "options": { "concurrency": 8 },
  "deep_crawl": {
    "max_depth": 2,
    "max_pages": 50
  }
}
```

### New Endpoints
```bash
# Streaming variants
POST /crawl/stream     # Server-Sent Events
POST /deepsearch/stream

# Enhanced headless rendering
POST /render           # Enhanced with new options
```

## Risk Assessment

### **Low Risk** ‚ö°
- Spider-rs integration (well-established library)
- PDF processing (pdfium-render already integrated)
- Streaming output (standard Axum SSE support)

### **Medium Risk** ‚ö†Ô∏è
- Stealth features (detection complexity)
- Performance with concurrent PDF processing
- Memory usage with deep crawling

### **Mitigation Strategies**
1. **Feature flags** for gradual rollout
2. **Circuit breakers** for PDF processing
3. **Memory monitoring** with alerts
4. **A/B testing** for stealth effectiveness

## Conclusion

The RipTide codebase demonstrates excellent architectural foundations for Phase 3 integration. The modular design, flexible pipeline system, and comprehensive monitoring infrastructure provide ideal integration points for all planned features.

**Key Success Factors:**
1. ‚úÖ Clean separation of concerns enables isolated feature development
2. ‚úÖ Existing gate system provides natural content routing mechanism
3. ‚úÖ WASM component architecture supports enhanced extraction
4. ‚úÖ Production-ready monitoring facilitates performance optimization

**Recommended Approach:**
- **Incremental integration** with feature flags
- **Extensive testing** at each phase
- **Performance monitoring** throughout implementation
- **Backward compatibility** preservation

The architecture review confirms that Phase 3 development can proceed with confidence, building upon the solid foundations established in Phases 1 and 2.